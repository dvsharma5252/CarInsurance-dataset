{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# To make data visualisations display in Jupyter Notebooks \n",
    "import numpy as np   # linear algebra\n",
    "import pandas as pd  # Data processing, Input & Output load\n",
    "import matplotlib.pyplot as plt # Visualization & plotting\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression #  Logistic Regression (aka logit) classifier in linear model\n",
    "import joblib  #Joblib is a set of tools to provide lightweight pipelining in Python (Avoid computing twice the same thing)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "                                    # GridSearchCV - Implements a “fit” and a “score” method\n",
    "                                    # train_test_split - Split arrays or matrices into random train and test subsets\n",
    "                                    # cross_val_score - Evaluate a score by cross-validation\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, make_scorer, accuracy_score, roc_curve, confusion_matrix, classification_report\n",
    "                                    # Differnt metrics to evaluate the model \n",
    "import pandas_profiling as pp   # simple and fast exploratory data analysis of a Pandas Datafram\n",
    "\n",
    "import warnings   # To avoid warning messages in the code run\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_train_actual, train_pred_prob, y_test_actual, test_pred_prob, *args):\n",
    "    '''\n",
    "    Generate the train & test roc curve\n",
    "    '''\n",
    "\n",
    "    AUC_Train = roc_auc_score(y_train_actual, train_pred_prob)\n",
    "    AUC_Test = roc_auc_score(y_test_actual, test_pred_prob)\n",
    "\n",
    "    if len(args) == 0:\n",
    "        print(\"Train AUC = \", AUC_Train)\n",
    "        print(\"Test AUC = \", AUC_Test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_train_actual, train_pred_prob)\n",
    "        fpr_tst, tpr_tst, thresholds = roc_curve(y_test_actual, test_pred_prob)\n",
    "        roc_plot(fpr, tpr, fpr_tst, tpr_tst)\n",
    "\n",
    "    else:\n",
    "        AUC_Valid = roc_auc_score(args[0], args[1])\n",
    "        print(\"Train AUC = \", AUC_Train)\n",
    "        print(\"Test AUC = \", AUC_Test)\n",
    "        print(\"Validation AUC = \", AUC_Valid)\n",
    "        fpr, tpr, thresholds = roc_curve(y_train_actual, train_pred_prob)\n",
    "        fpr_tst, tpr_tst, thresholds = roc_curve(y_test_actual, test_pred_prob)\n",
    "        fpr_val, tpr_val, thresholds = roc_curve(args[0], args[1])\n",
    "        roc_plot(fpr, tpr, fpr_tst, tpr_tst, fpr_val, tpr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(fpr, tpr, fpr_tst, tpr_tst, *args):\n",
    "    '''\n",
    "    Generates roc plot\n",
    "    '''\n",
    "\n",
    "    fig = plt.plot(fpr, tpr, label='Train')\n",
    "    fig = plt.plot(fpr_tst, tpr_tst, label='Test')\n",
    "\n",
    "    if len(args) == 0:\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.title(\"ROC curve using \")\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        fig = plt.plot(args[0], args[1], label='Validation')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.title(\"ROC curve using \")\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the dataset\n",
    "Insurance_Data = pd.read_csv('carInsurance_train.csv')\n",
    "print('Train Data Shape - ', Insurance_Data.shape)\n",
    "Insurance_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of values are stored in the columns?\n",
    "Insurance_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp.ProfileReport(Insurance_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some statistical information about our dataframe.\n",
    "Insurance_Data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we can get summary for the categorical data\n",
    "Insurance_Data.describe(include=np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = 'CarInsurance'\n",
    "pd.crosstab(Insurance_Data[Target], columns='N', normalize=True)\n",
    "# pd.crosstab(Insurance_Data[Target], columns='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count every row of class 1 as 2 rows of Class 1\n",
    "0.599/0.401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = Insurance_Data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_num_cols = Insurance_Data.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop columns which we will not use\n",
    "num_cols = Insurance_Data.drop(['Id', 'CarInsurance'],axis=1).select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_num_cols = Insurance_Data.drop(['CallStart', 'CallEnd'],axis=1).select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numeric Columns \\n', num_cols)\n",
    "print('Non-Numeric Columns \\n', non_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop CarLoan, HHInsurance, Default from the numeric columns as these are dummies\n",
    "num_cols_viz = ['DaysPassed', 'Age', 'NoOfContacts', 'PrevAttempts', 'LastContactDay', 'Balance']\n",
    "\n",
    "fig, axes = plt.subplots(3,2,sharex=False,sharey=False, figsize=(15,15))\n",
    "Insurance_Data.loc[:,[Target]+num_cols_viz].boxplot(by=Target, ax=axes, return_type='axes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "non_num_cols_viz = non_num_cols+['CarLoan', 'HHInsurance', 'Default']\n",
    "fig, axes = plt.subplots(len(non_num_cols_viz),sharex=False,sharey=False, figsize=(15,50))\n",
    "for i in range(len(non_num_cols_viz)):\n",
    "    pd.crosstab(Insurance_Data[non_num_cols_viz[i]], Insurance_Data[Target]).plot(kind='bar', \n",
    "                                                                                  stacked=True, \n",
    "                                                                                  grid=False, \n",
    "                                                                                  ax=axes[i],\n",
    "                                                                                  rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data_Org = Insurance_Data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data['Job'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data['Job'] = Insurance_Data['Job'].fillna('None')\n",
    "Insurance_Data['Job'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data['Job'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing education with the most common education level by job type\n",
    "\n",
    "# Create job-education level mode mapping\n",
    "edu_mode=[]\n",
    "\n",
    "# What are different Job Types\n",
    "job_types = Insurance_Data.Job.value_counts().index\n",
    "job_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now according to the job type we will crate a mapping where the job and mode of education is there.\n",
    "# It means when there are many people in the managment job then most of them are in which education.\n",
    "# We can find that in below mapping\n",
    "\n",
    "for job in job_types:\n",
    "    mode = Insurance_Data[Insurance_Data.Job==job]['Education'].value_counts().nlargest(1).index\n",
    "    edu_mode = np.append(edu_mode,mode)\n",
    "edu_map=pd.Series(edu_mode,index=Insurance_Data.Job.value_counts().index)\n",
    "\n",
    "edu_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping to missing education obs. We will replace education now by jobs value\n",
    "for j in job_types:\n",
    "    Insurance_Data.loc[(Insurance_Data['Education'].isnull()) & (Insurance_Data['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\n",
    "\n",
    "# For those who are not getting mapped we will create a new category as None\n",
    "Insurance_Data['Education'].fillna('None',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing communication with none \n",
    "Insurance_Data['Communication'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data['Communication'] = Insurance_Data['Communication'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing value in Outcome\n",
    "Insurance_Data['Outcome'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing outcome as not in previous campaign, we are adding one category to Outcome\n",
    "# We will add category if the value of DaysPassed is -1\n",
    "\n",
    "Insurance_Data.loc[Insurance_Data['DaysPassed']==-1,'Outcome']= 'NoPrev'\n",
    "Insurance_Data['Outcome'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have any missing values left\n",
    "Insurance_Data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data_num = Insurance_Data[num_cols+['Id', 'CarInsurance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns data\n",
    "Insurance_Data_cat = Insurance_Data[non_num_cols]\n",
    "non_num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies\n",
    "Insurance_Data_cat_dummies = pd.get_dummies(Insurance_Data_cat)  #One-Hot Embedding\n",
    "print(Insurance_Data_cat_dummies.shape)\n",
    "Insurance_Data_cat_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_Data_final = pd.concat([Insurance_Data_num, Insurance_Data_cat_dummies], axis=1)\n",
    "print(Insurance_Data_final.shape)\n",
    "Insurance_Data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are missing values before we run model\n",
    "Insurance_Data_final.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = Insurance_Data_final.drop(['Id', 'CarInsurance'], axis=1) #X\n",
    "train_label = Insurance_Data_final['CarInsurance'] #y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_state is the seed used by the random number generator. It can be any integer.\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, train_label, train_size=0.7 , stratify=train_label, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 10 rows of data, 7 rows class 0, 3 rows class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70-30 split, random splitting, train will have 7 rows(class 0), test will have 3 rows(class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify option will make sure that train has both the classes, and also test has both the classes in 70-30\n",
    "# Guarantee that:\n",
    "# train 7 rows(5 rows class 0, 2 rows class 1)\n",
    "# test 3 rows (2 rows class 0, 1 row of class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape[0]) # 2800, 1123 are 1s and the rest (2800-1123) 0s\n",
    "print(np.sum(y_train))\n",
    "print(y_test.shape[0]) # 1200, 481 are 1s and the rest (1200-481) 0s\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape - ', X_train.shape)\n",
    "print('Test shape  - ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model parameters to tune\n",
    "model_parameters = {\n",
    "        'penalty':['none'],\n",
    "#         'penalty':['l1', 'l2', None],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch the parameters to find the best parameters.\n",
    "model = LogisticRegression(random_state=1)\n",
    "\n",
    "gscv = GridSearchCV(estimator=model, \n",
    "                    param_grid=model_parameters, \n",
    "                    cv=5,  # 5-Fold Cross Validation\n",
    "                    verbose=1, #To print what it is doing\n",
    "                    n_jobs=-1, #fastest possible depending in the laptop\n",
    "                    scoring='f1') #tell us 12 f1-scores, 1 f1-score per combination\n",
    "\n",
    "gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best parameter are -', gscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gscv.best_score_)\n",
    "print(gscv.best_estimator_)\n",
    "print(gscv.scorer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit the model with the best parameters\n",
    "final_mod = LogisticRegression(**gscv.best_params_, random_state=1)\n",
    "final_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC on test by final_mod =', roc_auc_score(y_true=y_test,\n",
    "                                                        y_score=final_mod.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC on test by gscv =', roc_auc_score(y_true=y_test,\n",
    "                                                        y_score=gscv.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model coefficients\n",
    "list(zip(X_train.columns, final_mod.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "train_pred = final_mod.predict(X_train)\n",
    "test_pred = final_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report for train data is : \\n',\n",
    "      classification_report(y_train, train_pred))\n",
    "print('Classification report for test data is : \\n',\n",
    "      classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the variables used in the model as it will be required in future for new datasets prediction\n",
    "final_mod.variables = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(final_mod, 'best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC\n",
    "plt.subplots(figsize=(10, 5))\n",
    "train_prob = final_mod.predict_proba(X_train)[:, 1]\n",
    "test_prob = final_mod.predict_proba(X_test)[:, 1]\n",
    "\n",
    "plot_roc_curve(y_train, train_prob,\n",
    "               y_test, test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(list(final_mod.coef_.ravel()),\n",
    "                        list(X_train.columns)).reset_index()\n",
    "coefs.columns = ['feature', 'coefficient']\n",
    "coefs = coefs.sort_values(by='coefficient', ascending=False)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 5 features to display\n",
    "top_features = coefs[:5]\n",
    "top_features = top_features.sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "# Select bottom 5 features to display\n",
    "bottom_features = coefs[-5:]\n",
    "bottom_features = bottom_features.sort_values(\n",
    "    by='coefficient', ascending=False)\n",
    "\n",
    "# Display 10 most important features\n",
    "pd.concat([top_features, bottom_features], axis=0).plot(\n",
    "    kind='barh', figsize=(10, 5), x='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "\n",
    "best_model = joblib.load('best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "Insurance_test = pd.read_csv('carInsurance_test.csv')\n",
    "print('Test Data Shape  - ', Insurance_test.shape)\n",
    "Insurance_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values on the test data\n",
    "# The function takes the dataframe and does the same preprocessing that was done for train data\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    #Job \n",
    "    df['Job'] = df['Job'].fillna('None')\n",
    "    \n",
    "    #Education\n",
    "    # Apply the mapping to missing eductaion obs. We will replace education now by jobs value\n",
    "    for j in job_types:\n",
    "        df.loc[(df['Education'].isnull()) & (df['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\n",
    "\n",
    "    # For those who are not getting mapped we will create a new category as None\n",
    "    df['Education'] = df['Education'].fillna('None')\n",
    "    \n",
    "    #Communication\n",
    "    df['Communication'] = df['Communication'].fillna('None')\n",
    "    \n",
    "    #Outcome\n",
    "    df.loc[df['DaysPassed']==-1,'Outcome']='NoPrev'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_test_Org = Insurance_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the missing values the same we had done for Train\n",
    "Insurance_test = handle_missing_values(Insurance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Insurance_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical to dummies\n",
    "dummy_cols = pd.get_dummies(Insurance_test[non_num_cols])\n",
    "dummy_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the columns\n",
    "new_data = pd.concat([Insurance_test[num_cols], dummy_cols], axis=1)\n",
    "print(new_data.shape)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the variables of train are present in test\n",
    "# Variables in model\n",
    "best_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables missing in test data. This happens sometimes because of some categories not present in the new data\n",
    "vars_missing = list(set(best_model.variables) - set(new_data.columns))\n",
    "vars_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the missing columns in the dataset and fill them with 0\n",
    "# This will create columns bonly if there are missing values\n",
    "for i in vars_missing:\n",
    "    new_data[i] = 0\n",
    "    \n",
    "print(new_data.shape)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new dataset in the same order of the variables used in train\n",
    "new_data_final = new_data[best_model.variables]\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the new data\n",
    "new_data_final['Predicted'] = best_model.predict(new_data_final)\n",
    "new_data_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results\n",
    "new_data_final.to_csv('Predicted.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
